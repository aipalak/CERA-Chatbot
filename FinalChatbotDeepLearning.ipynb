{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FinalChatbotDeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kavyaaa-1/IT-Project_Chatbot/blob/main/FinalChatbotDeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "collapsed": true,
        "id": "HJfEOuM4HTFk"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import json\n",
        "import pickle\n",
        "import random\n",
        "import tflearn\n",
        "import string\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "stemmer =  LancasterStemmer()\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tflearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c2eZlhYbHgC",
        "outputId": "7af433cf-745d-41d0-d9a6-b83473978ad7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tflearn in /usr/local/lib/python3.7/dist-packages (0.5.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from tflearn) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.21.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from tflearn) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YGwYY_4PZcOw",
        "outputId": "acea604f-a19c-43c0-c81a-721d20946d8c"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open (\"intent.json\") as file:\n",
        "  data=json.load(file)\n",
        "try:\n",
        "  with open(\"data.pickle\",\"rb\") as f:\n",
        "    words,labels,training,output=pickle.load(f)\n",
        "except:\n",
        "  words=[]\n",
        "  labels=[]\n",
        "  docs_x=[]\n",
        "  docs_y=[]\n",
        "\n",
        "  for intent in data[\"intents\"]:\n",
        "    for pattern in intent[\"patterns\"]:\n",
        "      wrds=nltk.word_tokenize(pattern)\n",
        "      words.extend(wrds)\n",
        "      docs_x.append(wrds)\n",
        "      docs_y.append(intent[\"tag\"])\n",
        "\n",
        "    if intent[\"tag\"] not in labels:\n",
        "      labels.append(intent[\"tag\"])\n",
        "\n",
        "  words=[stemmer.stem(w.lower()) for w in words if w not in \"?\"]\n",
        "  words=sorted(list(set(words)))\n",
        "  labels=sorted(labels)\n",
        "  training=[]\n",
        "  output=[]\n",
        "  out_empty=[0 for _ in range(len(labels))]\n",
        "\n",
        "  for x,doc in enumerate(docs_x):\n",
        "    bag=[]\n",
        "\n",
        "    wrds=[stemmer.stem(w.lower()) for w in doc]\n",
        "    for w in words:\n",
        "      if w in wrds:\n",
        "        bag.append(1)\n",
        "      else:\n",
        "        bag.append(0)\n",
        "\n",
        "    output_row=out_empty[:]\n",
        "    output_row[labels.index(docs_y[x])]=1\n",
        "\n",
        "    training.append(bag)\n",
        "    output.append(output_row)\n",
        "\n",
        "  training=np.array(training)\n",
        "  output=np.array(output)\n",
        "\n",
        "  with open(\"data.picle\",\"wb\") as f:\n",
        "    pickle.dump((words,labels,training,output),f)\n",
        "\n",
        "ops.reset_default_graph()\n",
        "\n",
        "net=tflearn.input_data(shape=[None,len(training[0])])\n",
        "net=tflearn.fully_connected(net,8)\n",
        "net=tflearn.fully_connected(net,8)\n",
        "net=tflearn.fully_connected(net,len(output[0]),activation='softmax')\n",
        "net=tflearn.regression(net)\n",
        "\n",
        "model=tflearn.DNN(net)\n",
        "try:\n",
        "  model.load(\"model.tflearn\")\n",
        "except:\n",
        "  model=tflearn.DNN(net)\n",
        "  model.fit(training,output,n_epoch=1000,batch_size=8,show_metric=True)\n",
        "  model.save(\"model.tflearn\")\n",
        "\n",
        "    \n",
        "  "
      ],
      "metadata": {
        "id": "UrxXNiXTN3N1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58bb8db5-70e7-4ae0-b4a1-54d975498dfc"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Restoring parameters from /content/model.tflearn\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def bag_of_words(s,words):\n",
        "  bag=[0 for _ in range(len(words))]\n",
        "  s_words=nltk.word_tokenize(s)\n",
        "  s_words=[stemmer.stem(word.lower())for word in s_words]\n",
        "  for se in s_words:\n",
        "    for i,w in enumerate(words):\n",
        "      if w==se:\n",
        "        bag[i] = 1\n",
        "  return np.array(bag)"
      ],
      "metadata": {
        "id": "XIFC0stjJJRv"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "  print(\"Hey I am Cera!\")\n",
        "  while True:\n",
        "    inp=input(\"You:\")\n",
        "    if inp.lower()==\"quit\" or inp.lower()==\"bye\":\n",
        "      break\n",
        "\n",
        "    results=model.predict([bag_of_words(inp,words)])\n",
        "    results_index=np.argmax(results)\n",
        "    tag=labels[results_index]\n",
        "\n",
        "    for tg in data[\"intents\"]:\n",
        "      if tg[\"tag\"]==tag:\n",
        "        response=tg['responses']\n",
        "\n",
        "    corpus_text = listToString(response)\n",
        "    print(\"Cera: \"+ (answer(corpus_text,inp)))"
      ],
      "metadata": {
        "id": "hmSAmUTKT1fu"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def listToString(response):\n",
        "   \n",
        "    str1 = \" \"\n",
        "    return (str1.join(response))"
      ],
      "metadata": {
        "id": "xV9o1DnYQAz-"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "bhPshNwpTrIW"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def answer(corpus_text,inp):\n",
        "  corpus_sentences=nltk.sent_tokenize(corpus_text)\n",
        "  corpus_words=nltk.word_tokenize(corpus_text)\n",
        "\n",
        "  wn_lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "  def lemmatize_data(tokens):\n",
        "    return [wn_lemmatizer.lemmatize(token) for token in tokens]\n",
        "\n",
        "  punct_remover=dict((ord(punctuation),None) for punctuation in string.punctuation)\n",
        "\n",
        "  def get_processed_data(data):\n",
        "    return lemmatize_data(nltk.word_tokenize(data.lower().translate(punct_remover)))\n",
        "\n",
        "  corpus_sentences.append(inp)\n",
        "\n",
        "  word_vectorizer=TfidfVectorizer(tokenizer=get_processed_data)\n",
        "  corpus_word_vectors=word_vectorizer.fit_transform(corpus_sentences)\n",
        "  cos_sin_vectors=cosine_similarity(corpus_word_vectors[-1],corpus_word_vectors)\n",
        "  similar_response_idx=cos_sin_vectors.argsort()[0][-2]\n",
        "\n",
        "  return corpus_sentences[similar_response_idx]\n"
      ],
      "metadata": {
        "id": "qTiLfvkQO_Xu"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat()"
      ],
      "metadata": {
        "id": "3OnvmoHUg3XM",
        "outputId": "8d4fabbd-9a98-4243-d3a7-3f8819342160",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 66,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hey I am Cera!\n",
            "You:how often should i exfoliate?\n",
            "Cera: Most experts advise that you exfoliate two to three times per week — as long as your skin can handle it.\n",
            "You:side effects of retinol\n",
            "Cera: People who use retinols commonly experience dry and irritated skin, especially after using a new product.To lessen these side effects, try using your retinol every other night or every third night, and work your way up to using it nightly.\n",
            "You:bye\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "B5KIzojuhHNi"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}